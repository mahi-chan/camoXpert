# MoE System Verification Guide

## Overview

The CamoXpert model uses a **Mixture of Experts (MoE)** system with **content-aware routing** to select the best experts for each image. This document explains how the MoE system works and how to verify it's functioning correctly.

---

## How MoE Works in CamoXpert

### 1. **Content-Aware Routing**

The `ContentAwareGate` network analyzes image features from three perspectives:

```
Image Features â†’ ContentAwareGate â†’ Expert Selection Scores
                     â†“
              [Spatial Branch]  â† Detects edges, textures (local patterns)
              [Channel Branch]  â† Detects semantics (global context)
              [Max Branch]      â† Detects salient features (peaks)
                     â†“
                  Fusion â†’ Gate Logits [B, 7]
```

**Example**: An image with strong edges will produce high scores for `EdgeExpert`, while a texture-rich image will favor `TextureExpert`.

### 2. **Top-K Expert Selection**

For each image, the router selects the **top-3 best experts** based on gate scores:

```python
gate_logits = gate(features)          # [B, 7] - scores for all 7 experts
top_k_logits, top_k_indices = torch.topk(gate_logits, k=3)  # Select top-3
top_k_weights = F.softmax(top_k_logits)  # Convert to weights [B, 3]
```

**Key properties**:
- Each image gets its own top-3 experts (content-aware)
- Weights sum to 1.0 per image
- Experts are selected based on learned patterns, not random

### 3. **Expert Specialization**

CamoXpert has **7 specialized experts**:

| Expert | Specialization | When Selected |
|--------|---------------|---------------|
| **TextureExpert** | Fine-grained textures | Camouflaged objects with texture patterns |
| **AttentionExpert** | Spatial attention | Objects requiring focus refinement |
| **EdgeExpert** | Boundary detection | Clear object boundaries |
| **FrequencyExpert** | Multi-scale patterns | Objects with frequency characteristics |
| **HybridExpert** | Combined approaches | Complex scenes |
| **SemanticContextExpert** | Global context | Scene understanding |
| **ContrastExpert** | Low-contrast objects | Subtle camouflage |

### 4. **Weighted Output Combination**

Expert outputs are combined using learned weights:

```python
output = w1 * Expert1(x) + w2 * Expert2(x) + w3 * Expert3(x)
```

where `w1 + w2 + w3 = 1.0` and weights are learned per-image.

---

## Verification Tools

### Built-in Debug Mode (Always Active)

The MoE system includes **runtime verification checks** enabled by default:

```python
# In models/experts.py - MoELayer.forward()
if self.debug_mode:  # True by default
    # Check 1: Gate outputs are valid
    assert not torch.isnan(gate_logits).any()
    assert not torch.isinf(gate_logits).any()

    # Check 2: Weights sum to 1.0
    assert torch.allclose(weights.sum(dim=1), torch.ones(B))

    # Check 3: Weights are non-negative
    assert (weights >= 0).all()

    # Check 4: Output is valid
    assert not torch.isnan(output).any()
    assert output.abs().sum() > 1e-6  # Not all zeros
```

**What this catches**:
- âŒ NaN or Inf values (numerical instability)
- âŒ Invalid expert indices
- âŒ Weights that don't sum to 1.0
- âŒ All-zero outputs (experts not firing)

If training crashes with an assertion error, it means the MoE system has a bug.

### Comprehensive Verification Script

Run the standalone verification script to thoroughly test the MoE system:

```bash
cd /home/user/camoXpert

# Test with randomly initialized model (architecture check)
python scripts/verify_moe.py \
    --dataset-path /kaggle/input/cod10k-dataset/COD10K-v3 \
    --backbone edgenext_base \
    --num-experts 7 \
    --img-size 320 \
    --batch-size 4

# Test with trained checkpoint (routing quality check)
python scripts/verify_moe.py \
    --dataset-path /kaggle/input/cod10k-dataset/COD10K-v3 \
    --checkpoint /kaggle/working/checkpoints_sota/best_model.pth \
    --backbone edgenext_base \
    --num-experts 7 \
    --img-size 320 \
    --batch-size 4
```

**What it verifies**:

1. **[1/6] Routing Logic**
   - âœ… Top-3 selection works correctly
   - âœ… Indices are in valid range [0, 6]
   - âœ… No duplicate experts selected per image

2. **[2/6] Weight Properties**
   - âœ… Weights sum to 1.0 (within tolerance)
   - âœ… Weights are non-negative
   - âœ… Weights show diversity (not all equal)

3. **[3/6] Output Combination**
   - âœ… Output shapes match input shapes
   - âœ… Outputs are non-zero
   - âœ… Output magnitudes are reasonable

4. **[4/6] Numerical Stability**
   - âœ… No NaN or Inf in predictions
   - âœ… No extreme values (>1e6)

5. **[5/6] Content Awareness**
   - âœ… Router makes diverse decisions (not stuck on same experts)
   - âœ… Different images get different expert selections
   - ðŸ“Š Reports diversity ratio (higher = more content-aware)

6. **[6/6] Expert Specialization**
   - âœ… All experts are used (not completely ignored)
   - ðŸ“Š Shows expert usage distribution
   - âš ï¸ Warns if one expert dominates (>80%)

**Example output**:

```
ðŸ” MoE SYSTEM VERIFICATION
======================================================================

[1/6] Verifying routing logic...
  âœ… Routing logic verified: Top-3 selection working correctly

[2/6] Checking weight properties...
  âœ… Weight properties verified: Sum to 1.0, non-negative

[3/6] Verifying output combination...
  âœ… Output combination verified: Shapes correct, non-zero outputs

[4/6] Checking for numerical issues...
  âœ… Numerical stability verified: No NaN/Inf detected

[5/6] Verifying content-aware routing...
  ðŸ“Š Routing diversity: 127 unique combinations out of 240 decisions
     Diversity ratio: 52.92%
  âœ… High diversity - router is content-aware

[6/6] Analyzing expert specialization...

  ðŸ“Š Expert Selection Frequency:
     TextureExpert            :   142 ( 19.7%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     AttentionExpert          :   128 ( 17.8%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     EdgeExpert               :   119 ( 16.5%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     HybridExpert             :   113 ( 15.7%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     SemanticContextExpert    :   105 ( 14.6%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     FrequencyExpert          :    97 ( 13.5%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     ContrastExpert           :    16 (  2.2%) â–ˆ

  ðŸ“ˆ Load balance: Min=2.2%, Max=19.7%
  âœ… Experts show specialization with balanced load

======================================================================
ðŸ“‹ VERIFICATION SUMMARY
======================================================================

âœ“ All verification tests completed!

Key findings:
  â€¢ Routing diversity: 52.9% (higher is better)
  â€¢ Average weight std: 0.1245 (>0.05 indicates learning)
  â€¢ Average output magnitude: 0.3421

======================================================================
âœ… MoE system verification complete!
======================================================================
```

---

## What to Look For During Training

### 1. Expert Usage Statistics

The training script prints expert usage every epoch:

```
ðŸ“Š Expert Usage Statistics:
  TextureExpert            :   234 ( 21.2%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  AttentionExpert          :   198 ( 17.9%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  EdgeExpert               :   187 ( 16.9%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  HybridExpert             :   165 ( 14.9%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  ...
```

**Good signs**:
- âœ… All experts are used (>1% each)
- âœ… Usage varies between experts (specialization)
- âœ… No single expert dominates (>80%)

**Bad signs**:
- âš ï¸ One expert has 0% usage â†’ expert may be broken
- âš ï¸ One expert has >80% usage â†’ router not learning
- âš ï¸ All experts have ~14% usage â†’ perfectly uniform (router may be ignoring content)

### 2. Training Behavior

**Router is learning correctly if**:
- IoU improves steadily (e.g., 0.15 â†’ 0.30 â†’ 0.45 â†’ ...)
- Loss decreases consistently
- Expert usage distribution changes over epochs (router adapting)

**Router is NOT learning if**:
- IoU stays at 0.0000 despite loss decreasing
- Expert usage stays perfectly uniform (14.3% each)
- Training crashes with assertion errors

---

## Troubleshooting

### Issue: Assertion Error During Training

**Error**: `AssertionError: âŒ Weights don't sum to 1.0`

**Cause**: Numerical instability in softmax or gate network

**Fix**:
1. Reduce learning rate (too high LR causes instability)
2. Check for NaN gradients (`torch.isnan(model.parameters())`)
3. Enable gradient clipping (already enabled in train_ultimate.py)

---

### Issue: One Expert Dominates (>80%)

**Symptom**:
```
TextureExpert: 850 (85.2%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Others:        148 (14.8%)
```

**Cause**: Load balancing loss too weak (0.001)

**Fix**: Increase load balancing weight in `models/experts.py:391`:
```python
# Change from:
aux_loss = F.mse_loss(expert_freq, target_freq) * 0.001

# To:
aux_loss = F.mse_loss(expert_freq, target_freq) * 0.01
```

---

### Issue: All Experts Have Same Usage (~14.3%)

**Symptom**:
```
All experts:  142-145 (14.0-14.5%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

**Cause**: Load balancing loss too strong OR gate not learning

**Fix**:
1. **Check if router is learning**: Print gate weight gradients
2. **Reduce load balancing**: Change `0.001` â†’ `0.0001`
3. **Check gate learning rate**: Ensure gate network has sufficient LR

---

### Issue: IoU = 0.0000 Despite Training

**Symptom**: Loss decreasing but IoU stays at zero

**Cause**: Learning rate too high, model diverging

**Fix**: Reduce learning rate:
```bash
--lr 0.0001  # Instead of 0.00025
```

---

## Disabling Debug Mode

If debug checks slow down training (unlikely), disable them:

```python
# In models/camoxpert.py, when creating MoELayers:
self.moe_layers = nn.ModuleList([
    MoELayer(dim, num_experts=num_experts, top_k=top_k, debug_mode=False)
    for dim in self.feature_dims
])
```

**Warning**: Only disable if training speed is critical and MoE is verified working.

---

## Summary

The MoE system in CamoXpert:
1. âœ… Uses content-aware routing (analyzes features, not random)
2. âœ… Selects top-3 experts per image dynamically
3. âœ… Combines outputs with learned weights (sum to 1.0)
4. âœ… Has 7 specialized experts for different image characteristics
5. âœ… Includes runtime verification (catches bugs immediately)
6. âœ… Can be thoroughly tested with verification script

**To verify your model is working**:
```bash
# Run during or after training
python scripts/verify_moe.py \
    --dataset-path /kaggle/input/cod10k-dataset/COD10K-v3 \
    --checkpoint /kaggle/working/checkpoints_sota/best_model.pth \
    --num-experts 7 \
    --img-size 320
```

Look for:
- âœ… High routing diversity (>50%)
- âœ… Balanced expert usage (no expert <1% or >80%)
- âœ… All verification tests pass
